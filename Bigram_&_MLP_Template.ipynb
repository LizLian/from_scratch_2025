{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkSl1MfUZRBbat8LgKmCMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LizLian/from_scratch_2025/blob/main/Bigram_%26_MLP_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBIoOcdMXLlz"
      },
      "outputs": [],
      "source": [
        "# @title Bigram Template"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read in the name.txt file, build a dictionary to store the bigram and their count from the dataset.\n",
        "2. Sample from the model. Steps when encounters the end of sentence symbol.\n",
        "3. write the negative likelihood loss function. (cross entropy).\n",
        "4. convert the bigram model to neural net. (hint: convert x to one hot vectors, initialize weights, logits = xenc @ W. Update W.data in backward pass)\n",
        "5. sample the model again to see the results after optimization."
      ],
      "metadata": {
        "id": "We1bDvZJYkuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open(\"names.txt\") as infile:\n",
        "  for line in infile:\n",
        "    data.append(line.strip())"
      ],
      "metadata": {
        "id": "kgXot2Ojcj7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgdqgJTCgnZt",
        "outputId": "517bbc0c-8bd3-47fb-a8f7-3fa9494b9ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MLP Template"
      ],
      "metadata": {
        "id": "bXKqDWo3g2k6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the bigram model to MLP\n",
        "1. block size: context length - how many characters do we take to predict the next one?\n",
        "2. construct x and y training dataset (validation and test set too)\n",
        "3. hidden layer = emb @ W1 + b1, logits = h @ W2 + b2.\n",
        "4. compute loss\n",
        "5. collect all paramters, update each parameter's data.\n",
        "6. plot train loss with steps\n",
        "7. evaluate train loss and validation loss.\n",
        "8. sample from the model.\n"
      ],
      "metadata": {
        "id": "Rx_PDwUJ7ukp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kXpqbTZ7t_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}